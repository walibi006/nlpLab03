{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f142669",
   "metadata": {},
   "source": [
    "# NLP Lab 03 - Logistic regression classifier\n",
    "\n",
    "Authors:\n",
    "* Aurelien ROUXEL\n",
    "* Ethan MACHAVOINE\n",
    "* Jonathan POELGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b02de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datasets as ds\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a1784",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3349d28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "ds_train = ds.load_dataset(\"imdb\", split=\"train\")\n",
    "ds_test = ds.load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07e65a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, label_train = np.array(ds_train[\"text\"]), np.array(ds_train[\"label\"])\n",
    "text_test, label_test = np.array(ds_test[\"text\"]), np.array(ds_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7f8d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(base_text: str):\n",
    "  \"\"\"\n",
    "  Preprocess the text before classification\n",
    "  Args:\n",
    "    base_text: the string to preprocess\n",
    "  Return:\n",
    "    The preprocessed text\n",
    "  \"\"\"\n",
    "  base_text = base_text.lower()\n",
    "  base_text = base_text.replace(\"<br />\",' ')\n",
    "  text = \"\"\n",
    "  ponct = string.punctuation.replace(\"!\", '')\n",
    "  for char in base_text:\n",
    "    if char in ponct:\n",
    "      text += ' '\n",
    "    else:\n",
    "      text += char\n",
    "  return text\n",
    "\n",
    "vectorized_preprocessing = np.vectorize(preprocessing) \n",
    "\n",
    "text_train, text_test = vectorized_preprocessing(text_train), vectorized_preprocessing(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5edae2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vader_lexicon(filename = \"vader_lexicon.txt\"):\n",
    "    \"\"\"\n",
    "    Load the lexicon from VADER sentiment\n",
    "    Args:\n",
    "        filename: the name of file containing the VADER lexicon\n",
    "    Return\n",
    "        positive_lexicon: a numpy.array containg positive words\n",
    "        negative_lexicon: a numpy.array containg negative words\n",
    "    \"\"\"\n",
    "    loaded_lexicon = None\n",
    "    with open(filename, \"r\") as f:\n",
    "        loaded_lexicon = f.read()\n",
    "    if loaded_lexicon == None:\n",
    "        return None, None\n",
    "    positive_lexicon = []\n",
    "    negative_lexicon = []\n",
    "    for line in loaded_lexicon.rstrip('\\n').split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        (word, measure) = line.strip().split('\\t')[0:2]\n",
    "        measure = float(measure)\n",
    "        if (measure >= 1):\n",
    "            positive_lexicon.append(word)\n",
    "        elif (measure <= -1):\n",
    "            negative_lexicon.append(word)\n",
    "    return np.array(positive_lexicon), np.array(negative_lexicon)\n",
    "\n",
    "positive_lexicon, negative_lexicon = load_vader_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1eeaf1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Take a text and extract the features from it\n",
    "    Args:\n",
    "        text: the text to extract features from\n",
    "    Returns:\n",
    "        numpy.array of size 6 containing the features\n",
    "    \"\"\"\n",
    "    words = np.array(text.replace('!', '').split())\n",
    "    no_value = 0\n",
    "    if \"no\" in words:\n",
    "        no_value = 1\n",
    "    excla_value = 0\n",
    "    if '!' in text:\n",
    "        excla_value = 1\n",
    "    log_word_count = math.log(words.shape[0])\n",
    "    positive_words = words[np.isin(words, positive_lexicon)].shape[0]\n",
    "    negative_words = words[np.isin(words, negative_lexicon)].shape[0]\n",
    "    first_and_second_pronouns = np.concatenate((words[words == 'i'], words[words == 'you']), axis = 0).shape[0]\n",
    "    return np.array([no_value, first_and_second_pronouns, excla_value, \n",
    "                     log_word_count, positive_words, negative_words])\n",
    "\n",
    "vectorized_extract_features = np.vectorize(extract_features, signature='()->(6)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fac3c7",
   "metadata": {},
   "source": [
    "## Logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee92c9",
   "metadata": {},
   "source": [
    "### Applying the features extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "128615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorized_extract_features(text_train)\n",
    "X_test = vectorized_extract_features(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b3cc1",
   "metadata": {},
   "source": [
    "### Splitting training datase between training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67b05a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(label_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(label_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    stratify=y_train,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d4efc",
   "metadata": {},
   "source": [
    "### Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1b60b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"A logistic regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        output_layer = nn.Sigmoid() if nb_classes == 1 else nn.Softmax()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_dim, nb_classes),\n",
    "            output_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of activation function.\n",
    "        \"\"\"\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122689b",
   "metadata": {},
   "source": [
    "### Creating the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85a3a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(6, 1)\n",
    "criterion = nn.BCELoss()  # Binary cross entropy\n",
    "# Stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae2054",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbe54ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4128, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5858, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5827, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5819, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5815, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5813, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5812, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5812, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5811, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5811, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "CPU times: user 7.48 s, sys: 0 ns, total: 7.48 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "# Keeping an eye on the losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Setting all gradients to zero.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sending the whole training set through the model.\n",
    "    predictions = model(X_train)\n",
    "    # Computing the loss.\n",
    "    loss = criterion(predictions, y_train)\n",
    "    train_losses.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    # Computing the gradients and gradient descent.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # When computing the validation loss, we do not want to update the weights.\n",
    "    # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "    # gradient descent.\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_valid)\n",
    "        loss = criterion(predictions, y_valid)\n",
    "        test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ccb03",
   "metadata": {},
   "source": [
    "### Computing the accuracy for our 3 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d68bcb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7145882352941176\n",
      "Validation accuracy: 0.7058666666666666\n",
      "Test accuracy: 0.71348\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p_train = model(X_train)\n",
    "    p_train = np.round(p_train.numpy())\n",
    "    training_accuracy = np.mean(p_train == y_train.numpy())\n",
    "    p_valid = model(X_valid)\n",
    "    p_valid = np.round(p_valid.numpy())\n",
    "    valid_accuracy = np.mean(p_valid == y_valid.numpy())\n",
    "    p_test = model(X_test)\n",
    "    p_test = np.round(p_test.numpy())\n",
    "    test_accuracy = np.mean(p_test == y_test.numpy())\n",
    "print(f\"Training accuracy: {training_accuracy}\")\n",
    "print(f\"Validation accuracy: {valid_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdfbc5",
   "metadata": {},
   "source": [
    "Results:\n",
    "* Training accuracy: 0.7145882352941176\n",
    "* Validation accuracy: 0.7058666666666666\n",
    "* Test accuracy: 0.71348\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d5089",
   "metadata": {},
   "source": [
    "### Looking at the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cfa4fd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0281, -0.0488, -0.0070, -0.0132,  0.1276, -0.1493]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[0].state_dict()[\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33decc2",
   "metadata": {},
   "source": [
    "Weights:\n",
    "* tensor([[-0.0281, -0.0488, -0.0070, -0.0132,  0.1276, -0.1493]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf51b16",
   "metadata": {},
   "source": [
    "### Which features seems to play most for both classes ?\n",
    "\n",
    "    It seems that the features that play most for both classes are the count of positive words and negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc866c40",
   "metadata": {},
   "source": [
    "### Taking two wrongly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "13603eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0000,  5.0000,  1.0000,  5.1761, 11.0000,  7.0000]),\n",
       " tensor([0.0000, 3.0000, 0.0000, 4.8675, 7.0000, 2.0000])]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_test[i] for i in range(len(X_test)) if y_test[i] != torch.tensor(p_test)[i]][3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad3881",
   "metadata": {},
   "source": [
    "Wrongly classified:\n",
    "* tensor([ 0.0000,  5.0000,  1.0000,  5.1761, 11.0000,  7.0000])\n",
    "* tensor([0.0000, 3.0000, 0.0000, 4.8675, 7.0000, 2.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "62a8fbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.], dtype=float32), array([1.], dtype=float32)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p_test[i] for i in range(len(X_test)) if y_test[i] != torch.tensor(p_test)[i]][3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8bc2a",
   "metadata": {},
   "source": [
    "We can see that those two examples have been wronlgy classified as positives, now, let's take a look at a true positives and true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "106c5d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0000, 11.0000,  0.0000,  5.8665, 13.0000,  5.0000]),\n",
       " tensor([ 1.0000, 15.0000,  1.0000,  6.8773, 42.0000, 34.0000]),\n",
       " tensor([ 1.0000,  0.0000,  0.0000,  5.3519, 10.0000,  6.0000]),\n",
       " tensor([ 1.0000, 10.0000,  0.0000,  6.2166, 18.0000, 27.0000]),\n",
       " tensor([1.0000, 1.0000, 0.0000, 5.1240, 7.0000, 5.0000])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_test[i] for i in range(len(X_test)) if y_test[i] == 1][5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a3f51",
   "metadata": {},
   "source": [
    "True positives examples:\n",
    "* tensor([ 0.0000, 11.0000,  0.0000,  5.8665, 13.0000,  5.0000]),\n",
    "* tensor([ 1.0000, 15.0000,  1.0000,  6.8773, 42.0000, 34.0000]),\n",
    "* tensor([ 1.0000,  0.0000,  0.0000,  5.3519, 10.0000,  6.0000]),\n",
    "* tensor([ 1.0000, 10.0000,  0.0000,  6.2166, 18.0000, 27.0000]),\n",
    "* tensor([1.0000, 1.0000, 0.0000, 5.1240, 7.0000, 5.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "76137dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1.0000,  1.0000,  0.0000,  5.2470,  8.0000, 11.0000]),\n",
       " tensor([ 1.0000,  4.0000,  0.0000,  5.6937, 23.0000, 12.0000]),\n",
       " tensor([ 0.0000,  4.0000,  0.0000,  5.1533,  6.0000, 15.0000]),\n",
       " tensor([0.0000, 9.0000, 1.0000, 4.8675, 6.0000, 8.0000]),\n",
       " tensor([ 0.0000,  6.0000,  0.0000,  5.1417,  7.0000, 14.0000])]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_test[i] for i in range(len(X_test)) if y_test[i] == 0][5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faadc69",
   "metadata": {},
   "source": [
    "True negatives examples:\n",
    "* tensor([ 1.0000,  1.0000,  0.0000,  5.2470,  8.0000, 11.0000]),\n",
    "* tensor([ 1.0000,  4.0000,  0.0000,  5.6937, 23.0000, 12.0000]),\n",
    "* tensor([ 0.0000,  4.0000,  0.0000,  5.1533,  6.0000, 15.0000]),\n",
    "* tensor([0.0000, 9.0000, 1.0000, 4.8675, 6.0000, 8.0000]),\n",
    "* tensor([ 0.0000,  6.0000,  0.0000,  5.1417,  7.0000, 14.0000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc04d9",
   "metadata": {},
   "source": [
    "We can see that both wrong examples have more positive words than negative words, which has probably been recognized as a characteristic of positive comments, considering the that the count of positive and negative words seems to be the features that play most while looking at the model's weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
